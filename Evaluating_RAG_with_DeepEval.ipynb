{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXVsXEhtU2MUm/dNrN6jRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "368c3c6682644d4396d9929f66c88c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c00643510e444b6a36da0aacb221856",
              "IPY_MODEL_3e4e3e91b3dd44e9b838992fce3d2359",
              "IPY_MODEL_311792e7f9bd458ba06c89928b6cb317"
            ],
            "layout": "IPY_MODEL_19d4ef795fbd42f686e1bc8d8ef65f24"
          }
        },
        "1c00643510e444b6a36da0aacb221856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7c0297506644360aa6dec7fd76f6ca2",
            "placeholder": "​",
            "style": "IPY_MODEL_7dbfd16a9a7e41f799de5ad24dd282f5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3e4e3e91b3dd44e9b838992fce3d2359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac812c11a0764524a80d77df0bab4095",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dd9addd77de4d64b1df2067d6403f5c",
            "value": 4
          }
        },
        "311792e7f9bd458ba06c89928b6cb317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3278bd4200a2423e811a9d504f719c5a",
            "placeholder": "​",
            "style": "IPY_MODEL_99c45d5138a24fb5b81eeaa4ffbbe96f",
            "value": " 4/4 [01:21&lt;00:00, 17.07s/it]"
          }
        },
        "19d4ef795fbd42f686e1bc8d8ef65f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c0297506644360aa6dec7fd76f6ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbfd16a9a7e41f799de5ad24dd282f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac812c11a0764524a80d77df0bab4095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd9addd77de4d64b1df2067d6403f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3278bd4200a2423e811a9d504f719c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c45d5138a24fb5b81eeaa4ffbbe96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67711eb9705a4d64b7e6b350a9dc75dd": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f920af1639f6474f912ff89fc1b8e846",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠙\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Llama-3 8B, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠙</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Llama-3 8B, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f920af1639f6474f912ff89fc1b8e846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f491a12a444a42b796f6911713327845": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1bd341ab094e474cbd6601b8f912622b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠙\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBias Metric\u001b[0m! \u001b[38;2;55;65;81m(using Llama-3 8B, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠙</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Bias Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Llama-3 8B, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1bd341ab094e474cbd6601b8f912622b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6611cd0947b54b28a85f40d09514c08f": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1a95c7203099498c800a3ab9dc6f8253",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠹\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Llama-3 8B, strict=False, async_mode=False)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠹</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Llama-3 8B, strict=False, async_mode=False)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1a95c7203099498c800a3ab9dc6f8253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangcuongnguyen2001/RAG_lessons/blob/main/Evaluating_RAG_with_DeepEval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Đánh giá RAG (hoặc LLMs) với DeepEval**"
      ],
      "metadata": {
        "id": "s0UNbxvVbh7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhiều chương trình đã được phát triển để đánh giá RAG systems trong giai đoạn gần đây, như DeepEval, RAGAS và TRULens. Phần này chúng ta sẽ sử dụng DeepEval, một mô hình phát triển bởi start-up Confident AI: https://docs.confident-ai.com/.\n",
        "\n",
        "Lý do chúng ta sử dụng DeepEval trong topic này như sau:\n",
        "\n",
        "1. DeepEval là một mô hình mã nguồn mở, nên bất kì ai có thể sử dụng mà không cần mất phí (khác với các platforms của các công ty lớn như Amazon Bedrock).\n",
        "2. DeepEval hỗ trợ đo đạc nhiều metric khác nhau, như thiên vị (bias), độ độc hại của câu trả lời (toxicity), mức độ liên quan với câu hỏi (answer relevancy).\n",
        "3. Mọi người có thể sử dụng DeepEval chỉ với một câu hỏi và một câu trả lời từ LLM, thay vì dùng cả một dataset với một số platform khác.\n"
      ],
      "metadata": {
        "id": "jv49JSIMe2it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cài đặt các thư viện cần thiết:***\n",
        "\n"
      ],
      "metadata": {
        "id": "gboWHc4-zXb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chúng ta trước hết cần cài đặt các thư viện chính, để sử dụng DeepEval.\n",
        "\n",
        "Lưu ý là vì DeepEval cần một LLM nhất định, chúng ta cần phải cài thêm các thư viện cần thiết cho LLM, như `transformers`, `torch`, `bitsandbytes`, `accelerate`."
      ],
      "metadata": {
        "id": "nK6FF6kWzfNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIDrIGDVbrG",
        "outputId": "cfff6670-0e3e-4a22-aa24-74e2a7709a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepeval in /usr/local/lib/python3.10/dist-packages (0.21.74)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepeval) (4.66.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.12.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from deepeval) (13.7.1)\n",
            "Requirement already satisfied: protobuf==4.25.1 in /usr/local/lib/python3.10/dist-packages (from deepeval) (4.25.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.8.2)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.11.0)\n",
            "Requirement already satisfied: pytest-repeat in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.3)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.6.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.10.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.2.11)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.2.25)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.1.19)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.1.12)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.0.0)\n",
            "Requirement already satisfied: tenacity~=8.4.1 in /usr/local/lib/python3.10/dist-packages (from deepeval) (8.4.2)\n",
            "Requirement already satisfied: opentelemetry-api==1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-sdk==1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.24.0)\n",
            "Requirement already satisfied: grpcio==1.63.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.63.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api==1.24.0->deepeval) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->deepeval) (1.63.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->deepeval) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->deepeval) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk==1.24.0->deepeval) (0.45b0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk==1.24.0->deepeval) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval) (3.19.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (0.1.94)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->deepeval) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core->deepeval) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval) (2024.7.4)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai->deepeval) (1.37.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai->deepeval) (0.7.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.1)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.10/dist-packages (from pytest-xdist->deepeval) (2.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (2.20.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.2.10)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval) (2.16.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval) (1.5.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval) (1.9.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api==1.24.0->deepeval) (1.14.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->deepeval) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->deepeval) (3.10.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->deepeval) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (1.3.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->deepeval) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai->deepeval) (2024.5.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->ragas->deepeval) (2024.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.23.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval) (0.6.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai->deepeval) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas->deepeval) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ba3s_clVhLA",
        "outputId": "847b75a6-11fe-4585-f08f-a1e750ff76ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với LLMs, formats DeepEval cần sử dụng để đọc input là JSON. Có một số trường hợp do model quá cũ, prompt engineering sẽ không thể tạo ra JSON format phù hợp. Chúng ta có thể gặp lỗi sau đây:\n",
        "\n",
        "`ValueError: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.`\n",
        "\n",
        "Do đó, chúng ta cần một phương pháp để thay đổi kết quả của các LLMs cho phù hợp, bằng thư viện `lm-format-enforcer`.\n",
        "\n",
        "*Lưu ý:* Với các mô hình của các hãng lớn (GPT-4, Gemini,...) vấn đề này sẽ không xảy ra, nên chúng ta có thể bỏ qua bước này.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6eS1d8Rz7vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-format-enforcer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs8VvG14Y4IP",
        "outputId": "dda4f09a-a2f8-4b68-8ff4-7d8cd79e1ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lm-format-enforcer in /usr/local/lib/python3.10/dist-packages (0.10.5)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (24.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (6.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bước tiếp theo của chúng ta là tải LLMs xuống GPU cho DeepEval sử dụng. Chúng ta sẽ tuân theo các bước sau:\n",
        "\n",
        "1. Thừa kế DeepEvalBaseLLM.\n",
        "2. Sử dụng `get_model_name()` method. Nó sẽ trả về tên của model chúng ta đang sử dụng (trong ví dụ này, chúng ta sẽ dùng Llama 3).\n",
        "3. Sử dụng `load_model()` method, nhằm trả về một đối tượng (object) liên quan tới model.\n",
        "4. Sử dụng generate() method, với 2 input chính: string (prompt), và schema (từ BaseModel). Lưu ý: BaseModel được tạo bởi thư viện pydantic, một thư viện tạo kiểu dữ liệu phổ biến trên Python.\n",
        "5. Chúng ta gọi parser để thay đổi prompt và kết quả cho phù hợp với yêu cầu của DeepEval, sử dụng JsonSchemaParser từ lm-format-enforcer.\n",
        "6. Một method khác, a_generate(prompt) cần được viết, giống như generate(prompt), nhưng diễn ra không đồng thời với method gốc (nên được gọi là asynchronous (async) method). Trong ví dụ này, method này chỉ dùng lại generate(prompt), nhưng các bạn có thể thay đổi để cho việc đánh giá LLMs nhanh hơn.\n",
        "\n",
        "Các bạn có thể tham khảo post này của DeepEval để tìm hiểu thêm: https://docs.confident-ai.com/docs/guides-using-custom-llms"
      ],
      "metadata": {
        "id": "bGJXxT6o0-1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTa6wiIZU33h"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from pydantic import BaseModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import (\n",
        "    build_transformers_prefix_allowed_tokens_fn,\n",
        ")\n",
        "import json\n",
        "from deepeval.models import DeepEvalBaseLLM\n",
        "\n",
        "\n",
        "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
        "    def __init__(self):\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "        )\n",
        "\n",
        "        self.model = model_4bit\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
        "        model = self.load_model()\n",
        "\n",
        "        pipeline = transformers.pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            use_cache=True,\n",
        "            device_map=\"auto\",\n",
        "            max_length=2500,\n",
        "            do_sample=True,\n",
        "            top_k=5,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        # Create parser required for JSON confinement using lmformatenforcer\n",
        "        parser = JsonSchemaParser(schema.schema())\n",
        "        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
        "            pipeline.tokenizer, parser\n",
        "        )\n",
        "\n",
        "        # Output and load valid JSON\n",
        "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
        "        output = output_dict[0][\"generated_text\"][len(prompt) :]\n",
        "        json_result = json.loads(output)\n",
        "\n",
        "        # Return valid JSON object according to the schema DeepEval supplied\n",
        "        return schema(**json_result)\n",
        "\n",
        "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
        "        return self.generate(prompt, schema)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Llama-3 8B\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Một số ví dụ về metric dùng trong đánh giá RAG:***"
      ],
      "metadata": {
        "id": "BDpqJJoS3nFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Mức độ liên quan với câu hỏi (Answer Relevancy Metric):*"
      ],
      "metadata": {
        "id": "rXdUQN2S4DwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là một metric đo xem câu trả lời `(actual_output)` của RAG có liên quan gì tới câu hỏi `(input)` hay không. Nó là một số thập phân từ 0 tới 1, đo bằng cách sau đây:\n",
        "\n",
        "\n",
        "> Answer Relevancy Metric = Số lượng câu trả về liên quan tới câu hỏi/Tổng số câu trả về từ RAG.\n",
        "\n",
        "DeepEval sẽ dùng LLM (trong ví dụ này là Llama3) để tìm tất cả các câu trong `actual_output`, rồi xếp hạng xem nó có liên quan tới `input` hay không.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G-dGWcRm4NAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Lưu ý:** Có 6 parameters cần thiết cho metric này, chúng ta có thể thêm hoặc không:\n",
        "\n",
        "- [Optional] threshold: Một số thập phân cho số nhâ a float representing the minimum passing threshold, defaulted to 0.5.\n",
        "[Optional] model: a string specifying which of OpenAI's GPT models to use, OR any custom LLM model of type DeepEvalBaseLLM. Defaulted to 'gpt-4o'.\n",
        "[Optional] include_reason: a boolean which when set to True, will include a reason for its evaluation score. Defaulted to True.\n",
        "[Optional] strict_mode: a boolean which when set to True, enforces a binary metric score: 1 for perfection, 0 otherwise. It also overrides the current threshold and sets it to 1. Defaulted to False.\n",
        "[Optional] async_mode: a boolean which when set to True, enables concurrent execution within the measure() method. Defaulted to True.\n",
        "[Optional] verbose_mode: a boolean which when set to True, prints the intermediate steps used to calculate said metric to the console, as outlined in the How Is It Calculated section. Defaulted to False."
      ],
      "metadata": {
        "id": "EM0eIJee61B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# evaluate a single test case\n",
        "actual_output = r'''The Transformer model architecture is described in the paper \"Attention is All You Need\" [1]. The model consists of an encoder and a decoder,\n",
        "both of which are composed of stacked self-attention and point-wise, fully connected layers. The encoder has six identical layers, each with two sub-layers:\n",
        "a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The decoder has an additional sub-layer that performs\n",
        "multi-head attention over the output of the encoder stack. Residual connections and layer normalization are employed around each sub-layer. The attention function\n",
        " maps a query and a set of key-value pairs to an output, which is a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\n",
        "The model achieves state-of-the-art results on several natural language processing tasks, including machine translation and constituency parsing.\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "custom_llm = CustomLlama3_8B()\n",
        "metric = AnswerRelevancyMetric(\n",
        "    threshold=0.7,\n",
        "    model=custom_llm,\n",
        "    include_reason=True\n",
        ")\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Could you summarize the model architecture of Transformers based on 'Attention is all you need' paper?\",\n",
        "    actual_output=actual_output\n",
        ")\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "368c3c6682644d4396d9929f66c88c5d",
            "1c00643510e444b6a36da0aacb221856",
            "3e4e3e91b3dd44e9b838992fce3d2359",
            "311792e7f9bd458ba06c89928b6cb317",
            "19d4ef795fbd42f686e1bc8d8ef65f24",
            "e7c0297506644360aa6dec7fd76f6ca2",
            "7dbfd16a9a7e41f799de5ad24dd282f5",
            "ac812c11a0764524a80d77df0bab4095",
            "4dd9addd77de4d64b1df2067d6403f5c",
            "3278bd4200a2423e811a9d504f719c5a",
            "99c45d5138a24fb5b81eeaa4ffbbe96f",
            "67711eb9705a4d64b7e6b350a9dc75dd",
            "f920af1639f6474f912ff89fc1b8e846"
          ]
        },
        "id": "oT-9yuA6VCwv",
        "outputId": "6fcf3e0e-2714-4dab-aa2f-0b199b3e5dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368c3c6682644d4396d9929f66c88c5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67711eb9705a4d64b7e6b350a9dc75dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "The score is 1.00 because the actual output directly addresses the question and provides a concise summary of the Transformer model architecture, with no irrelevant statements or information that is not related to the topic. The output is highly relevant to the input and accurately answers the question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BiasMetric\n",
        "\n",
        "\n",
        "metric = BiasMetric(threshold=0.5, model=custom_llm)\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Could you summarize the model architecture of Transformers based on 'Attention is all you need' paper?\",\n",
        "    # Replace this with the actual output from your LLM application\n",
        "    actual_output=actual_output\n",
        ")\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "f491a12a444a42b796f6911713327845",
            "1bd341ab094e474cbd6601b8f912622b"
          ]
        },
        "id": "StPQcFIrWX8p",
        "outputId": "09c389f4-7896-4e1f-9a6a-92192cc03bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f491a12a444a42b796f6911713327845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import HallucinationMetric\n",
        "\n",
        "# Replace this with the actual documents that you are passing as input to your LLM.\n",
        "context=[r'''In addition, we are taking actions to optimize our global office space. As a result we expect to incur exit costs\n",
        "relating to office space reductions of approximately $0.5 billion in the first quarter of 2023. We may incur\n",
        "additional charges in the future as we further evaluate our real estate needs.\n",
        "•In January 2023, we completed an assessment of the useful lives of our servers and network equipment,\n",
        "resulting in a change in the estimated useful life of our servers and certain network equipment to six years,\n",
        "which we expect to result in a reduction of depreciation of approximately $3.4 billion for the full fiscal year 2023\n",
        "for assets in service as of December 31, 2022, recorded primarily in cost of revenues and R&D expenses.\n",
        "•As AI is critical to delivering our mission of bringing our breakthrough innovations into the real world, beginning\n",
        "in January 2023, we will update our segment reporting relating to certain of Alphabet's AI activities. DeepMind,\n",
        "previously reported within Other Bets, will be reported as part of Alphabet's corporate costs, reflecting its\n",
        "increasing collaboration with Google Services, Google Cloud, and Other Bets. Prior periods will be recast to\n",
        "conform to the revised presentation. See Note 15 of the Notes to Consolidated Financial Statements included\n",
        "in Item 8 of this Annual Report on Form 10-K for information relating to our segments.\n",
        "\n",
        "''']\n",
        "\n",
        "# Replace this with the actual output from your LLM application\n",
        "actual_output=r'''Alphabet in 2023 had to deal with these challenges: Pausing activities and reduce costs\n",
        " (Alphabet needs to reduce the global office space in approximately 0.5 billion USD in 2023);\n",
        " assessing the useful lives of its server and network equipment, with an estimated reduction in depreciation expenses\n",
        "  of approximately $3.4 billion in fiscal 2023. It also have to revalue and reorganize AI operations,\n",
        "   including DeepMind, to reflect increased collaboration between AI operations and other operations.\n",
        "\n",
        "'''\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Could you summarize challenges that Alphabet is facing in 2023?\",\n",
        "    actual_output=actual_output,\n",
        "    context=context\n",
        ")\n",
        "metric = HallucinationMetric(threshold=0.5, model=custom_llm, include_reason=True )\n",
        "\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72,
          "referenced_widgets": [
            "6611cd0947b54b28a85f40d09514c08f",
            "1a95c7203099498c800a3ab9dc6f8253"
          ]
        },
        "id": "Y3JSfzSfUzz9",
        "outputId": "843c3387-0dc8-4d45-b8d8-bf6c5694b30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6611cd0947b54b28a85f40d09514c08f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3333333333333333\n",
            "The score is 0.33 because the actual output lacks detail about the actions taken to optimize the global office space and change in estimated useful life of servers and network equipment, which are mentioned in the context, but the actual output agrees with the context on other points, indicating some hallucination has occurred, but not extensively so, resulting in a relatively low hallucination score of 0.33.\n"
          ]
        }
      ]
    }
  ]
}