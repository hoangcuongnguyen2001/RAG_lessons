{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7MrQMyPtv7Ao6Q/xTjpBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "080b6ff1f0d74fc6b7cad70a042fdb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5417746d324d4091a24cae373fe75d41",
              "IPY_MODEL_868be9606ad9459e83105a1165e86ed8",
              "IPY_MODEL_5972d4096a604648831ed5819cc5737a"
            ],
            "layout": "IPY_MODEL_44e0889551344c468ee1d1720a2a5868"
          }
        },
        "5417746d324d4091a24cae373fe75d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68f452e2dd84c9493be737e356d6424",
            "placeholder": "​",
            "style": "IPY_MODEL_8591f3eca9cd4ba490839a9b8c132b6c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "868be9606ad9459e83105a1165e86ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93f4331960114c7da3a983ece4e3970c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59b48a43486f4aea9192a50d1fa7b7dd",
            "value": 4
          }
        },
        "5972d4096a604648831ed5819cc5737a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a8eaa601df498495179385909161ce",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c190f38b7e43d4aa3adc7a0ed69044",
            "value": " 4/4 [01:17&lt;00:00, 16.80s/it]"
          }
        },
        "44e0889551344c468ee1d1720a2a5868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68f452e2dd84c9493be737e356d6424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8591f3eca9cd4ba490839a9b8c132b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93f4331960114c7da3a983ece4e3970c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b48a43486f4aea9192a50d1fa7b7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6a8eaa601df498495179385909161ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c190f38b7e43d4aa3adc7a0ed69044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8970fd5893ae4072bcf72b6717b2cba1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3561ef9ff4f74615a2f889a507f7c1a1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": " ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Llama 3, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Llama 3, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3561ef9ff4f74615a2f889a507f7c1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangcuongnguyen2001/RAG_lessons/blob/main/RAG_lesson_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG pipeline cơ bản (với LlamaIndex và Llama 3)**"
      ],
      "metadata": {
        "id": "uDKNK6u0frKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là notebook cho các bạn để có thể tìm hiểu về cách tạo ra một mô hình RAG đơn giản, với LlamaIndex và open-source LLM như Llama 3.\n",
        "\n",
        "Lời cảm ơn đặc biệt dành cho Plaban Nayak: [Build an Advanced Reranking-RAG System Using Llama-Index, Llama 3 and Qdrant](https://medium.com/@nayakpplaban/build-an-advanced-reranking-rag-system-using-llama-index-llama-3-and-qdrant-a8b8654174bc), vì ý tưởng cho notebook này."
      ],
      "metadata": {
        "id": "lTumNrjhf7rK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cài đặt các thư viện cần thiết:***"
      ],
      "metadata": {
        "id": "lS55MdeAgmQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trước tiên, việc cần làm đầu tiên là cài đặt các thư viện cần thiết cho RAG pipeline. Một số điều cần lưu ý:\n",
        "- LlamaIndex là framework chính chúng ta sẽ sử dụng, nó có hỗ trợ cho người dùng với kết nối LLM và các mô hình cho embeddings.\n",
        "- Embeddings và LLM ở đây đều là mô hình mã nguồn mở, nên các mô hình này sẽ cần HuggingFace tokens (chìa khóa để kết nối với HuggingFace).\n",
        "- sentence-transformers là phiên bản riêng để kích hoạt thư viện transformers cho xử lý câu trong NLP."
      ],
      "metadata": {
        "id": "DlZ16Wligxz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "llama-index\n",
        "llama-index-llms-huggingface\n",
        "llama-index-embeddings-fastembed\n",
        "bitsandbytes\n",
        "llama-index-embeddings-huggingface\n",
        "fastembed\n",
        "deepeval\n",
        "einops\n",
        "accelerate\n",
        "sentence-transformers"
      ],
      "metadata": {
        "id": "lymECqAKbDIM",
        "outputId": "276151fd-2d99-4716-919f-842be299fafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "sDbnfnolbIir",
        "outputId": "6e6ac103-5c69-4eb7-fd28-2a2ce3a15890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.10.51)\n",
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.5)\n",
            "Requirement already satisfied: llama-index-embeddings-fastembed in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.4)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.43.1)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: fastembed in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.2.7)\n",
            "Requirement already satisfied: deepeval in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.21.62)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.31.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.0.1)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.51 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.10.51)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.24)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.25)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.35.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub<0.21.0,>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (4.39.3)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 6)) (1.16.1)\n",
            "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 6)) (1.18.1)\n",
            "Requirement already satisfied: tokenizers<0.16,>=0.15 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 6)) (0.15.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (7.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.9.0)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.12.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (13.7.1)\n",
            "Requirement already satisfied: protobuf==4.25.1 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (4.25.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (2.7.4)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (2.7.1)\n",
            "Requirement already satisfied: pytest-repeat in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.9.3)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (3.6.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.2.6)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.2.10)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.1.13)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.1.9)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (0.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (7.1.0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 9)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 9)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 9)) (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 10)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 10)) (1.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.21.0,>=0.20.3->llama-index-llms-huggingface->-r requirements.txt (line 2)) (3.15.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval->-r requirements.txt (line 7)) (3.19.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index->-r requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed->-r requirements.txt (line 6)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed->-r requirements.txt (line 6)) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed->-r requirements.txt (line 6)) (1.12.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval->-r requirements.txt (line 7)) (1.63.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval->-r requirements.txt (line 7)) (1.64.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval->-r requirements.txt (line 7)) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.14.0->deepeval->-r requirements.txt (line 7)) (0.46b0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval->-r requirements.txt (line 7)) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2024.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval->-r requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval->-r requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval->-r requirements.txt (line 7)) (0.1.82)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->deepeval->-r requirements.txt (line 7)) (1.33)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval->-r requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.10/dist-packages (from pytest-xdist->deepeval->-r requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval->-r requirements.txt (line 7)) (0.2.6)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval->-r requirements.txt (line 7)) (0.3.4)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval->-r requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval->-r requirements.txt (line 7)) (2.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 10)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 10)) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval->-r requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->deepeval->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->deepeval->-r requirements.txt (line 7)) (3.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->deepeval->-r requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed->-r requirements.txt (line 6)) (10.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (3.21.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval->-r requirements.txt (line 7)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval->-r requirements.txt (line 7)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval->-r requirements.txt (line 7)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval->-r requirements.txt (line 7)) (0.70.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.51->llama-index->-r requirements.txt (line 1)) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "dDNdfV95LtXa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Nhập tài liệu (Loading):***"
      ],
      "metadata": {
        "id": "v1Bo_40ViM85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo đó, các bạn cần nhập tài liệu vào trong vector database.\n",
        "\n",
        "Ở notebook này, tài liệu chúng ta lựa chọn là một trích đoạn trong bản dịch tiếng Việt của \"Trận chiến cuối cùng\", một tác phẩm về trận Berlin trong thế chiến II: [Wikipedia](https://en.wikipedia.org/wiki/The_Last_Battle_(Ryan_book)).\n",
        "\n",
        "Tài liệu sẽ được đưa vào VectorStoreIndex (vector database cơ bản nhất của LlamaIndex), để lưu trữ cho query của các bạn."
      ],
      "metadata": {
        "id": "xOwWQPVeikEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/hoangcuongnguyen2001/RAG_lessons/main/Last_battle.txt\" \"Last_battle.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoWLKHZtHwct",
        "outputId": "141060c1-3d73-465f-cf95-66d2201d0d9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-01 06:23:05--  https://raw.githubusercontent.com/hoangcuongnguyen2001/RAG_lessons/main/Last_battle.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11281 (11K) [text/plain]\n",
            "Saving to: ‘Last_battle.txt.1’\n",
            "\n",
            "\rLast_battle.txt.1     0%[                    ]       0  --.-KB/s               \rLast_battle.txt.1   100%[===================>]  11.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 06:23:06 (123 MB/s) - ‘Last_battle.txt.1’ saved [11281/11281]\n",
            "\n",
            "--2024-07-01 06:23:06--  http://last_battle.txt/\n",
            "Resolving last_battle.txt (last_battle.txt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘last_battle.txt’\n",
            "FINISHED --2024-07-01 06:23:06--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 11K in 0s (123 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"Last_battle.txt\"]\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "FDdmxH3HH500"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo đó, các bạn có thể cài đặt system prompt (prompt hướng dẫn cho LLM nên làm gì), cùng với query wrapper prompt (để quy định cấu trúc cho query của người hỏi trước khi nhập vào LLM). Lưu ý, query wrapper prompt của mỗi LLM có thể tìm được trên model card tại HuggingFace; và trong ví dụ này mình sử dụng wrapper prompt từ Llama-3-8B-Instruct model card. Argument này có thể không cần thiết với các model khác.\n",
        "\n",
        "Một điều đáng lưu ý nữa là nếu không có system prompt, hành vi ngầm định khi hỏi Llama3 bằng tiếng Việt là nó có thể thêm tiếng Anh trong câu trả lời."
      ],
      "metadata": {
        "id": "mEY7dVYKj1CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "system_prompt = \"\"\"Bạn là một trợ lý AI đắc lực. Hãy trả lời các câu hỏi càng chính xác càng tốt.\n",
        " Lưu ý là bạn chỉ trả lời bằng tiếng Việt và không dùng ngôn ngữ nào khác. Hãy in ra kết quả dưới dạng JSON,\n",
        " với <result></result> tags ở đầu và cuối câu trả lời của bạn.\n",
        "\"\"\"\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "tgOrfFcS7gMR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Indexing và lưu trữ tài liệu:***"
      ],
      "metadata": {
        "id": "4OHBZkj1l8wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi nhập tài liệu vào database, việc tiếp theo sẽ là tạo embedding cho từng phần nhỏ một (chunk), trước khi nhập embedding vào vector store index (đã được kích hoạt trước đó).\n",
        "\n",
        "Lưu ý: embedding model chúng ta cần dùng nên là một mô hình hỗ trợ embedding cho nhiều ngôn ngữ (ở đây là *multilingual-e5-base*), hoặc một mô hình được huấn luyện với tài liệu của ngôn ngữ chúng ta muốn dùng trong câu hỏi. Với công nghệ hiện tại, chúng ta chưa thể chọn một mô hình embedding bất kỳ cho một LLM bất kỳ được."
      ],
      "metadata": {
        "id": "ZVDuMDQ-mFne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-base\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "UP6ogLD5H-vK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo đó, tokenizer và LLM sẽ được tải lên cùng nhau (ở đây cả tokenizer và LLM đều từ chung một mô hình).\n",
        "\n",
        "Stoppings_ids ở đây được sử dụng để giúp cho một LLM có thể hiểu được về việc kết thúc của từng câu là gì (nó tùy thuộc vào cấu trúc của từng LLM, như trong notebook này thì stopping_ids có thể tìm trong model card)."
      ],
      "metadata": {
        "id": "YKbzWMsTn8eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "stopping_ids = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSUtqtNLMkjf",
        "outputId": "da4b7f54-b408-4c20-d1ce-8ceaad0c4e70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lưu ý khi nhập và setup LLM:*\n",
        "\n",
        "1. Do môi trường làm việc của chúng ta là bản free của Google Colab (chỉ có 15GB RAM), các bạn nên sử dụng quantization (4 hoặc 8 bit) tùy thuộc vào kích cỡ LLM, để có thể nhập được model vào server.\n",
        "\n",
        "2. Các bạn có thể setup temperature trong ngưỡng từ 0 tới 1, nên để temperature ở mức gần 0 nhất có thể để đảm bảo cho kết quả được chính xác.\n",
        "\n",
        "3. Context windowm, chunk size và max new tokens ở đây đều là cài đặt ngầm định của Llama3-8B-Instruct."
      ],
      "metadata": {
        "id": "DV_MwwSpo0XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    context_window=8192,\n",
        "    max_new_tokens=256,\n",
        "    model_kwargs={\n",
        "        \"token\": hf_token,\n",
        "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
        "        \"quantization_config\": quantization_config\n",
        "    },\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.3,\n",
        "\n",
        "    },\n",
        "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    stopping_ids=stopping_ids,\n",
        "    response_format= \"json\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uwso8rJtUBnZ",
        "outputId": "4d9cc030-a6ea-4b62-b2be-683ee9f8aa34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "positional argument follows keyword argument (<ipython-input-17-d851b2c7f204>, line 34)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-d851b2c7f204>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    response_format: \"json\",\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "Settings.chunk_size = 512\n",
        "# Llama-3-8B-Instruct model\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "IDNxvU0-IMdW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp theo, embedding sẽ được lưu trữ vào VectorStoreIndex (với LlamaIndex)."
      ],
      "metadata": {
        "id": "6Oa5Pw4bq8GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "48CgUXG9JCrI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Đặt câu hỏi cho LLM (querying):***"
      ],
      "metadata": {
        "id": "DnzB2ZyBrTJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bước cuối cùng là đặt câu hỏi cho LLM thông qua vector database (như trong ví dụ này, chúng ta có thể đặt câu hỏi thông qua VectorStoreIndex (as_query_engine).\n",
        "\n",
        "Lưu ý: Chúng ta có thể điều chỉnh similarity_top_k (top bao nhiêu từ mà LLM đoán là có khả năng nối tiếp từ cũ). Thông thường, similarity_top_k càng thấp thì kết quả đưa ra càng chính xác."
      ],
      "metadata": {
        "id": "epMcJuSorca3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=3, streaming=True)\n",
        "#query_engine = index.as_query_engine(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "6SCXHeF9JK9H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Chuyện gì đã xảy ra với Sở thú Berlin?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhxWV1pDJQPK",
        "outputId": "7ebc5832-ca27-4f84-bc56-cff86a81d230"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu trả lời có thể được in trực tiếp (dùng *print()*), hoặc in theo từng từ một, giống như cách ChatGPT hoặc các LLM khác đưa ra câu trả lời (dùng *print_response_stream()*)."
      ],
      "metadata": {
        "id": "lj8MX_JVsG7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#response.print_response_stream()\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZm8k65fJfnb",
        "outputId": "0b77c63a-aeab-4f3f-ebed-8579891fc818"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sở thú Berlin đã bị hư hại nghiêm trọng, giờ chỉ còn là một khu rừng đổ nát. Hồ cá đã bị hủy hoại hoàn toàn. Các khu nuôi bò sát, hà mã, kangaroo, hổ và voi cũng bị hư hại nghiêm trọng, cùng với các tòa nhà bị thủng toang hoác khác.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "\n",
        "class Llama3(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        model = self.load_model()\n",
        "\n",
        "        device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        generated_ids = model.generate(**model_inputs, max_new_tokens=256, do_sample=True)\n",
        "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Llama 3\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             trust_remote_code=True,\n",
        "                                              quantization_config=quantization_config\n",
        "    )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "llama3 = Llama3(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Nig94yfPzcOW",
        "outputId": "d5125d71-3995-4f18-dea2-8b0a0c96795e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "080b6ff1f0d74fc6b7cad70a042fdb1b",
            "5417746d324d4091a24cae373fe75d41",
            "868be9606ad9459e83105a1165e86ed8",
            "5972d4096a604648831ed5819cc5737a",
            "44e0889551344c468ee1d1720a2a5868",
            "e68f452e2dd84c9493be737e356d6424",
            "8591f3eca9cd4ba490839a9b8c132b6c",
            "93f4331960114c7da3a983ece4e3970c",
            "59b48a43486f4aea9192a50d1fa7b7dd",
            "a6a8eaa601df498495179385909161ce",
            "d6c190f38b7e43d4aa3adc7a0ed69044"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "080b6ff1f0d74fc6b7cad70a042fdb1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama3.generate(\"What have happened in Berlin Zoo in 1945?\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "FHlv8GeFB0jF",
        "outputId": "6a0d112f-403e-4e5e-b4b3-f380e31c08a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>What have happened in Berlin Zoo in 1945? - History\n",
            "Berlin Zoo, which is one of the oldest and most famous zoos in the world, has a dark and tragic history during World War II. In 1945, the zoo was severely damaged and many animals died due to the war and the subsequent Soviet occupation.\n",
            "In the final months of World War II, Berlin was heavily bombed by the Allies, and the zoo was not spared. On February 3, 1945, a British air raid damaged the zoo's buildings and killed many animals. The zoo's director, Lutz Heck, was away at the time, and his wife, Margarete, was left in charge of the zoo.\n",
            "As the Soviet army approached Berlin, the zoo's staff and animals were in danger. Margarete Heck and her team worked tirelessly to care for the remaining animals, but it was a difficult and desperate situation. Many animals had died due to the bombing, and the zoo's resources were dwindling.\n",
            "On April 20, 1945, the Soviet army occupied Berlin, and the zoo was taken over by the Red Army. The zoo's staff was arrested and imprisoned, and many of the animals were killed or taken away to be used in Soviet zoos.\n",
            "The zoo's buildings were severely damaged, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import ContextualRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "actual_output = [\"\"\"\n",
        "Sở thú Berlin đã bị hư hại nghiêm trọng. Các khu nuôi bò sát, hà mã, kangaroo, hổ và voi đã bị hư hại\n",
        "nghiêm trọng, cùng với các tòa nhà bị thủng toang hoác khác. Hồ cá đã bị hủy hoại hoàn toàn.\n",
        "\n",
        "\"\"\"]\n",
        "\n",
        "\n",
        "retrieval_context = [\"\"\"\n",
        "Cách đó gần 100m là Sở thú Berlin danh tiếng thế giới, giờ chỉ còn là một khu rừng đổ nát.\n",
        "Hồ cá đã bị hủy hoại hoàn toàn. Các khu nuôi bò sát, hà mã, kangaroo, hổ và voi cũng bị hư hại nghiêm trọng,\n",
        " cùng với các tòa nhà bị thủng toang hoác khác. Công viên Tiergarten nổi tiếng rộng 630 mau Anh nằm xung quanh\n",
        "  đó giờ thành vùng đất không người, chỉ còn trơ lại những hố bom to bằng cả căn phòng, những hồ nước đầy gạch\n",
        "  vụn và tòa đại sứ đã hư hại một phần. Công viên từng là một khu rừng thiên nhiên đầy kỳ hoa di thảo. Giờ\n",
        "   những cái cây quý giá ấy đã bị thiêu trụi, trơ lại mây gốc cây xấu xí.\n",
        "\"\"\"]\n",
        "metric = ContextualRelevancyMetric(\n",
        "    threshold=0.7,\n",
        "    model=llama3,\n",
        "    include_reason=True\n",
        ")\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Chuyện gì đã xảy ra với Sở thú Berlin?\",\n",
        "    actual_output=actual_output,\n",
        "    retrieval_context=retrieval_context\n",
        ")\n",
        "\n",
        "print(metric)\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)\n",
        "\n",
        "# # or evaluate test cases in bulk\n",
        "# evaluate([test_case], [metric])"
      ],
      "metadata": {
        "id": "ukFqCbne0AHX",
        "outputId": "911bc656-3c66-470d-fadc-4ee0dcb9a6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "8970fd5893ae4072bcf72b6717b2cba1",
            "3561ef9ff4f74615a2f889a507f7c1a1"
          ]
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8970fd5893ae4072bcf72b6717b2cba1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<deepeval.metrics.contextual_relevancy.contextual_relevancy.ContextualRelevancyMetric object at 0x7d2cf018ae30>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/utils.py\u001b[0m in \u001b[0;36mtrimAndLoadJson\u001b[0;34m(input_string, metric)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonStr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 1 (char 222)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b5d05f02e268>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py\u001b[0m in \u001b[0;36mmeasure\u001b[0;34m(self, test_case)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_or_create_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 loop.run_until_complete(\n\u001b[0m\u001b[1;32m     67\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py\u001b[0m in \u001b[0;36ma_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m    102\u001b[0m         ):\n\u001b[1;32m    103\u001b[0m             self.verdicts: List[ContextualRelevancyVerdict] = (\n\u001b[0;32m--> 104\u001b[0;31m                 await self._a_generate_verdicts(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py\u001b[0m in \u001b[0;36m_a_generate_verdicts\u001b[0;34m(self, text, retrieval_context)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimAndLoadJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0mverdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContextualRelevancyVerdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mverdicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepeval/metrics/utils.py\u001b[0m in \u001b[0;36mtrimAndLoadJson\u001b[0;34m(input_string, metric)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"An unexpected error occurred: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model."
          ]
        }
      ]
    }
  ]
}